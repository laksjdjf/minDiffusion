{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1188de-9d2e-4830-b2aa-ed606d65d6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (1.11.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy) (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8ecd8c-a114-4cf9-bd29-618df47a244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        files = os.listdir(path)\n",
    "        self.file_list = [os.path.join(path,file) for file in files]\n",
    "        self.transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.Resize(64),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "         ]\n",
    "    )\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.file_list[i])\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce74bfd-16ad-4dd6-b643-ee8ba40de19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "from sympy import Ci\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from mindiffusion.unet import NaiveUnet\n",
    "from mindiffusion.ddpm import DDPM\n",
    "\n",
    "def train_anime(\n",
    "    n_epoch: int = 100, device: str = \"cuda:0\", load_pth: Optional[str] = None\n",
    ") -> None:\n",
    "\n",
    "    ###設定############\n",
    "    n_feat = 640\n",
    "    batch_size = 128\n",
    "    lr = 5e-5\n",
    "    dataset_dir = \"/storage/animeface/images/\"\n",
    "    ###################\n",
    "\n",
    "    ddpm = DDPM(eps_model=NaiveUnet(3, 3, n_feat=n_feat), betas=(1e-4, 0.02), n_T=1000)\n",
    "\n",
    "    if load_pth is not None:\n",
    "        ddpm.load_state_dict(torch.load(load_pth))\n",
    "\n",
    "    ddpm.to(device)\n",
    "\n",
    "    dataset = AnimeDataset(dataset_dir)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    optim = torch.optim.Adam(ddpm.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        print(f\"Epoch {i} : \")\n",
    "        ddpm.train()\n",
    "\n",
    "        pbar = tqdm(dataloader)\n",
    "        loss_ema = None\n",
    "        for x in pbar:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            loss = ddpm(x)\n",
    "            loss.backward()\n",
    "            if loss_ema is None:\n",
    "                loss_ema = loss.item()\n",
    "            else:\n",
    "                loss_ema = 0.9 * loss_ema + 0.1 * loss.item()\n",
    "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "            optim.step()\n",
    "\n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            xh = ddpm.sample(8, (3, 32, 32), device)\n",
    "            xset = torch.cat([xh, x[:8]], dim=0)\n",
    "            grid = make_grid(xset, normalize=True, value_range=(-1, 1), nrow=4)\n",
    "            save_image(grid, f\"./contents/ddpm_sample_anime{str(i).zfill(3)}.png\")\n",
    "\n",
    "            # save model\n",
    "            torch.save(ddpm.state_dict(), f\"./ddpm_anime{i%3}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d383e50-e61b-4870-9487-84c95ea4c764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.1019:   3%|▎         | 13/497 [00:36<20:34,  2.55s/it]"
     ]
    }
   ],
   "source": [
    "train_anime(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0926a6f-9636-4d23-921c-7b71316b79ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
